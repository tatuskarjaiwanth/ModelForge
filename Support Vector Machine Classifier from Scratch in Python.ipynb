{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"10amh3u9OhefFB0C-3lJo2H53x41I9GnJ","timestamp":1720606472090}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OoQnMmCCBfTF"},"source":["**SVM Classifier**"]},{"cell_type":"markdown","metadata":{"id":"VgNJ4FGKBkIu"},"source":["Equation of the Hyperplane:\n","\n","**y = wx - b**"]},{"cell_type":"markdown","metadata":{"id":"X8OJXGuPvDt2"},"source":["**Gradient Descent:**\n","\n","Gradient Descent is an optimization algorithm used for minimizing the loss function in various machine learning algorithms. It is used for updating the parameters of the learning model.\n","\n","w  =  w - α*dw\n","\n","b  =  b - α*db"]},{"cell_type":"markdown","metadata":{"id":"WSAfYP7WmECB"},"source":["**Learning Rate:**\n","\n","Learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."]},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"8LN3z4maI7tl","executionInfo":{"status":"ok","timestamp":1720631930261,"user_tz":-330,"elapsed":450,"user":{"displayName":"Jaiwanth Tatuskar","userId":"02152202838956048967"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class SVM_classifier():\n","    def __init__(self , learning_rate , no_of_iterations , lambda_parameter):\n","        self.learning_rate = learning_rate\n","        self.no_of_iterations = no_of_iterations\n","        self.lambda_parameter = lambda_parameter\n","\n","    def fit(self, X , y):\n","        self.m , self.n = X.shape #m= no of rows , n = number of input features(columns)\n","        self.w = np.zeros(self.n)\n","        self.b = 0\n","        self.X = X\n","        self.y = y\n","        for i in range(self.no_of_iterations):\n","            self.update_weights()\n","\n","    def update_weights(self):\n","        y_label = np.where(self.y <=0 , -1 , 1) #0 , 1 dosent work with svm so we change it to 1 , -1\n","\n","        for index , x_i in enumerate(self.X):\n","\n","            condition = y_label[index] * (np.dot(x_i , self.w)-self.b) >= 1\n","            if condition==True:\n","                dw = 2*self.lambda_parameter*self.w\n","                db = 0\n","            else:\n","                dw = 2*self.lambda_parameter*self.w - np.dot(x_i , y_label[index])\n","                db = y_label[index]\n","        self.w = self.w - self.learning_rate*dw\n","        self.b = self.b - self.learning_rate*db\n","\n","\n","    def predict(self , X):\n","        output = np.dot(X , self.w) - self.b\n","        predicted_labels = np.sign(output)\n","        y_hat = np.where(predicted_labels <= -1 , 0 , 1)\n","        return y_hat\n","\n","\n","\n"],"metadata":{"id":"MfWvuqUpI7v_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = SVM_classifier(learning_rate=0.001 , no_of_iterations=1000 , lambda_parameter=0.01)"],"metadata":{"id":"VMAglKmoI7yg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2oHHgtVPI70q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qs0GLrfuI74J"},"execution_count":null,"outputs":[]}]}